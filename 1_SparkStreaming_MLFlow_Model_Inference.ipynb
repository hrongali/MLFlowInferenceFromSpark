{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0ee48-2546-46d1-8119-4536cc69f870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct Cloud Storage URL is:s3a://hrong1-cdp-bucket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to hrongali\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import datetime\n",
    "import yaml\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, FloatType, StructField, StructType, DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf, array, col\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import struct\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "#Extracting the correct URL from hive-site.xml\n",
    "tree = ET.parse('/etc/hadoop/conf/hive-site.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "for prop in root.findall('property'):\n",
    "    if prop.find('name').text == \"hive.metastore.warehouse.dir\":\n",
    "        storage = prop.find('value').text.split(\"/\")[0] + \"//\" + prop.find('value').text.split(\"/\")[2]\n",
    "\n",
    "print(\"The correct Cloud Storage URL is:{}\".format(storage))\n",
    "\n",
    "os.environ['STORAGE'] = storage\n",
    "\n",
    "spark = SparkSession.builder.appName('MLFlow model inference').config(\"spark.yarn.access.hadoopFileSystems\",os.environ[\"STORAGE\"]).getOrCreate()\n",
    "\n",
    "predict = mlflow.pyfunc.spark_udf(spark, \"/home/cdsw/model\")\n",
    "\n",
    "\n",
    "customSchema = StructType() \\\n",
    "     .add(\"_c0\", FloatType(), True) \\\n",
    "     .add(\"_c1\", FloatType(), True) \\\n",
    "     .add(\"_c2\", FloatType(), True) \\\n",
    "     .add(\"_c3\", FloatType(), True) \\\n",
    "     .add(\"_c4\", FloatType(), True) \\\n",
    "     .add(\"_c5\", FloatType(), True) \\\n",
    "     .add(\"_c6\", FloatType(), True) \\\n",
    "     .add(\"_c7\", FloatType(), True) \\\n",
    "     .add(\"_c8\", FloatType(), True) \\\n",
    "     .add(\"_c9\", FloatType(), True) \\\n",
    "     .add(\"_c10\", FloatType(), True) \\\n",
    "     .add(\"_c11\", FloatType(), True) \\\n",
    "     .add(\"_c12\", FloatType(), True) \\\n",
    "     .add(\"_c13\", FloatType(), True) \\\n",
    "     .add(\"_c14\", FloatType(), True) \\\n",
    "     .add(\"_c15\", FloatType(), True) \\\n",
    "     .add(\"_c16\", FloatType(), True) \\\n",
    "     .add(\"_c17\", FloatType(), True) \\\n",
    "     .add(\"_c18\", FloatType(), True) \\\n",
    "     .add(\"_c19\", FloatType(), True) \\\n",
    "     .add(\"_c20\", FloatType(), True) \\\n",
    "     .add(\"_c21\", FloatType(), True) \\\n",
    "     .add(\"_c22\", FloatType(), True) \\\n",
    "     .add(\"_c23\", FloatType(), True) \\\n",
    "     .add(\"_c24\", FloatType(), True)\n",
    "\n",
    "\n",
    "df_data=spark.readStream.format('csv').option('header',False).option(\"sep\",\",\").schema(customSchema).load(os.environ[\"STORAGE\"]+\"/tmp/\")\n",
    "\n",
    "df=df_data.select(struct([df_data[i] for i in range(25)]).alias('domain_tokens'))\n",
    "#df.show()\n",
    "df1=df.withColumn(\"prediction\", predict(df.domain_tokens)).withColumn('timestamp', F.current_timestamp()).withWatermark(\"timestamp\", \"3 seconds\").groupBy(\"domain_tokens\",\"timestamp\").sum(\"prediction\")\n",
    "df1.writeStream.format('json').outputMode(\"append\").trigger(processingTime='2 seconds').option(\"checkpointLocation\",os.environ[\"STORAGE\"]+\"/chkpnt/\").option(\"path\",os.environ[\"STORAGE\"]+\"/output/\").start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120290aa-aaaa-4e0d-9271-422ae62bee1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a6f8ac-32d1-402e-affd-e373ac723b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d0d5cd-5657-419e-8db5-813d3cedd0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74542d87-4af8-419e-a3be-d71b1fedf8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e72f3-d047-4a7f-919b-2900a3e2019a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107341a8-cfe3-4321-bbf2-9d6b2d791e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d222807e-482b-4d32-8008-194eb84c2d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b37f01-fcec-409f-b16c-9a91e7402b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
